---
title: Optimizing Polkadot’s Data Availability
description: Optimizing Polkadot’s data availability with erasure coding, systematic chunks, and chunk rotation for scalability and efficiency.
sidebar:
    label: Kamil Shalakiev (Kagome)
    order: 7
---

import { Image } from 'astro:assets';
import coverImage from '/src/assets/sub0-reset/kamil-shalakiev-kagome.webp';

<Image src={coverImage} alt="kamil-shalakiev-kagome" />

The Sub0 Reset 2024 event showcased some of the latest innovations within the Polkadot ecosystem, with a focus on advancing blockchain scalability and efficiency. One of the standout talks, delivered by **Kamil Salakhiev**, CEO and Engineering Manager at Kagome, delved into **Polkadot’s Data Availability (DA)**. This session revealed the intricacies of optimizing data availability and recovery within Polkadot's architecture, shedding light on the math and mechanics underpinning DA while comparing Polkadot’s approach with other prominent DA solutions, such as **Celestia** and **Avail**.

This article explores the essential concepts discussed, including **erasure coding**, **systematic chunks**, and **chunk rotation**, along with their role in improving scalability and efficiency in Polkadot’s ecosystem.

---

## Why Is Data Availability Critical?

In Polkadot’s architecture, **data availability** ensures that parachain blocks are accessible for verification and execution. This mechanism is vital for maintaining Polkadot’s performance, especially as it scales to support more than 300 parachains.

Key objectives for optimizing data availability include:
1. **Fast Finality:** Ensuring parachain blocks are validated and finalized within Polkadot’s 12-second window.
2. **Bandwidth Efficiency:** Managing the high traffic associated with transactions across hundreds of parachains.
3. **High Availability Guarantees:** Achieving near-100% certainty that the necessary data is accessible for execution.

Without an optimized DA layer, Polkadot’s capacity to process transactions and validate state transitions would be severely constrained, hindering scalability and user experience.

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vSnZs_GAH1WUh6BIREjVISaQuAkLdo1iPb5BS5Cu8ramxyLEcq_3IA5fK2RCy4G7c6CltUlohvMF9sB/embed?start=false&loop=false&delayms=3000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>


## Comparing Polkadot’s DA to Other Solutions

Polkadot’s DA strategy differs from those employed by **optimistic rollups** and **validity rollups**:
- **Optimistic Rollups**: Submit data to Layer 1 for verification over a dispute window (e.g., seven days). Fraud proofs validate state transitions when disputes arise.
- **Validity Rollups (ZK Rollups)**: Submit proof of execution (e.g., ZK-SNARKs) and data to Layer 1 for deterministic verification.
- **Polkadot**: Relies on relay chain validators to verify and recover data, ensuring high performance and fast finality. The validators actively participate in verifying the **proof of validity (PoV)** and maintaining the data availability pipeline.

This approach integrates directly with Polkadot’s shared security model, leveraging relay chain validators to streamline the DA process.

---

## Erasure Coding and Data Distribution

At the core of Polkadot’s DA optimization lies **erasure coding**, a technique for splitting and distributing data among validators:
1. **Initial Step:** Parachain data is encoded using **Reed-Solomon erasure coding**. The process transforms data of size *N* into **3N+1 chunks**, which are distributed to validators.
2. **Chunk Distribution:** Each validator receives a unique chunk. No single validator has access to the entire data, enhancing decentralization and resilience.
3. **Reconstruction:** Any *N* chunks are sufficient to reconstruct the original data using polynomial interpolation. This ensures data availability even if some validators fail.

While erasure coding adds redundancy, it significantly reduces storage and bandwidth requirements compared to traditional replication methods.

---

## Systematic Chunks and Chunk Rotation

To further enhance efficiency, Polkadot employs **systematic chunks** and **chunk rotation**:
- **Systematic Chunks:** The first *N* chunks in the erasure coding process directly correspond to the original data. Validators with these chunks can reconstruct the data without performing costly polynomial interpolation.
- **Chunk Rotation:** Introduced via RFC 47, chunk rotation ensures that the assignment of chunks to validators changes dynamically. This prevents overloading specific validators with repeated requests, balancing network traffic and improving scalability.

---

## Optimizing Data Recovery

Polkadot’s validators employ various strategies for data recovery:
1. **Requesting Data from Backing Validators:** Validators responsible for creating the erasure-coded data can provide the full dataset. However, this approach is limited to smaller data sizes (e.g., under 128 KB) to avoid bandwidth congestion.
2. **Reconstructing from Chunks:** For larger datasets, validators fetch a subset of chunks and use systematic reconstruction to recover the data.
3. **Approvals and Disputes:** Additional validators validate the recovered data to ensure its integrity. In cases of disputes, all validators reconstruct the data to verify its validity.

These methods collectively ensure that Polkadot maintains high throughput while minimizing computational overhead.

---

## Comparing Polkadot with Celestia and Avail

While Polkadot’s DA mechanism is highly optimized for its relay chain and parachain architecture, Celestia and Avail offer distinct approaches:
- **Celestia:** Focuses on modular blockchains, separating consensus and data availability. Validators verify DA proofs without directly reconstructing the data.
- **Avail:** Provides DA for rollups and standalone chains, emphasizing scalability and ease of integration.

Polkadot’s integration of DA within its relay chain ensures tight coupling with its shared security model, whereas Celestia and Avail prioritize modularity and interoperability.

<iframe width="560" height="315" src="https://www.youtube.com/embed/dsI7JS447AY?si=7Q5jgMejrHoG2eNk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

## Innovations from Kagome

Kamil Salakhiev’s team at Kagome has played a pivotal role in advancing Polkadot’s DA capabilities:
- Developed a **C++ implementation of Polkadot** for enhanced performance.
- Implemented custom optimizations for erasure coding and data recovery, achieving faster reconstruction speeds compared to existing libraries.
- Contributed to the adoption of systematic chunks and chunk rotation in the Polkadot ecosystem.

These contributions highlight Kagome’s commitment to pushing the boundaries of blockchain scalability and efficiency.

---

## Conclusion

Polkadot’s data availability framework exemplifies its commitment to scalability, security, and efficiency. By leveraging advanced techniques like erasure coding, systematic chunks, and chunk rotation, Polkadot ensures fast finality and high availability for parachain data. Comparisons with solutions like Celestia and Avail underscore Polkadot’s unique position as a robust, relay-chain-centric platform.

As Polkadot continues to evolve, innovations from teams like Kagome will play a vital role in refining its infrastructure and setting new benchmarks for blockchain performance. The advancements discussed at Sub0 Reset 2024 reaffirm Polkadot’s leadership in the Web3 ecosystem.
